{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text        label\n",
      "0      Stuning even for the non-gamer: This sound tr...  __label__2 \n",
      "1      The best soundtrack ever to anything.: I'm re...  __label__2 \n",
      "2      Amazing!: This soundtrack is my favorite musi...  __label__2 \n",
      "3      Excellent Soundtrack: I truly like this sound...  __label__2 \n",
      "4      Remember, Pull Your Jaw Off The Floor After H...  __label__2 \n",
      "...                                                 ...          ...\n",
      "9995   A revelation of life in small town America in...  __label__2 \n",
      "9996   Great biography of a very interesting journal...  __label__2 \n",
      "9997   Interesting Subject; Poor Presentation: You'd...  __label__1 \n",
      "9998   Don't buy: The box looked used and it is obvi...  __label__1 \n",
      "9999   Beautiful Pen and Fast Delivery.: The pen was...  __label__2 \n",
      "\n",
      "[10000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "Corpus = pd.read_csv('dataset.csv', encoding=\"latin-1\")\n",
    "print(Corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus['text'].dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus['text'] = [entry.lower() for entry in Corpus['text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus['text']= [word_tokenize(entry) for entry in Corpus['text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag_map defaultdict(<function <lambda> at 0x1a1c608c20>, {'J': 'a', 'V': 'v', 'R': 'r'})\n"
     ]
    }
   ],
   "source": [
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "print(\"tag_map\", tag_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text        label  \\\n",
      "0  [stuning, even, for, the, non-gamer, :, this, ...  __label__2    \n",
      "1  [the, best, soundtrack, ever, to, anything, .,...  __label__2    \n",
      "2  [amazing, !, :, this, soundtrack, is, my, favo...  __label__2    \n",
      "3  [excellent, soundtrack, :, i, truly, like, thi...  __label__2    \n",
      "4  [remember, ,, pull, your, jaw, off, the, floor...  __label__2    \n",
      "\n",
      "                                          text_final  \n",
      "0  ['stun', 'even', 'sound', 'track', 'beautiful'...  \n",
      "1  ['best', 'soundtrack', 'ever', 'anything', 're...  \n",
      "2  ['amaze', 'soundtrack', 'favorite', 'music', '...  \n",
      "3  ['excellent', 'soundtrack', 'truly', 'like', '...  \n",
      "4  ['remember', 'pull', 'jaw', 'floor', 'hear', '...  \n"
     ]
    }
   ],
   "source": [
    "for index,entry in enumerate(Corpus['text']):\n",
    "\n",
    "    Final_words = []\n",
    "\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "\n",
    "    for word, tag in pos_tag(entry):\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    Corpus.loc[index,'text_final'] = str(Final_words)\n",
    "print(Corpus.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(Corpus['text_final'],Corpus['label'],test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectorization ( Tf-Idf )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = LabelEncoder()\n",
    "Train_Y = Encoder.fit_transform(Train_Y)\n",
    "Test_Y = Encoder.fit_transform(Test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    ['stun', 'even', 'sound', 'track', 'beautiful'...\n",
      "1    ['best', 'soundtrack', 'ever', 'anything', 're...\n",
      "2    ['amaze', 'soundtrack', 'favorite', 'music', '...\n",
      "3    ['excellent', 'soundtrack', 'truly', 'like', '...\n",
      "4    ['remember', 'pull', 'jaw', 'floor', 'hear', '...\n",
      "Name: text_final, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(Corpus['text_final'][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_fit = Tfidf_vect.fit(Corpus['text_final'])\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the vectorizer locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(Tfidf_fit,open(\"feature2.pkl\",\"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  83.8\n"
     ]
    }
   ],
   "source": [
    "model = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto', probability=True)\n",
    "model.fit(Train_X_Tfidf,Train_Y)\n",
    "predictions_SVM = model.predict(Test_X_Tfidf)\n",
    "print(\"Accuracy: \",accuracy_score(predictions_SVM, Test_Y)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open(\"saved_model\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing new sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing dataframe. Needs to be done to match with the input parameter of Tfidf_vect\n",
    "p = pd.DataFrame({\"new_text\":[]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            new_text\n",
      "0  ['fascinating', 'insight', 'life', 'modern', '...\n"
     ]
    }
   ],
   "source": [
    "def preprocessTesting(sentence):\n",
    "\n",
    "    sentence = [entry.lower() for entry in sentence]\n",
    "    sentence = [word_tokenize(entry) for entry in sentence]\n",
    "    \n",
    "    for index,entry in enumerate(sentence):\n",
    "\n",
    "        Final_words = []\n",
    "\n",
    "        word_Lemmatized = WordNetLemmatizer()\n",
    "\n",
    "        for word, tag in pos_tag(entry):\n",
    "            if word not in stopwords.words('english') and word.isalpha():\n",
    "                word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "                Final_words.append(word_Final)\n",
    "        p.loc[index,'new_text'] = str(Final_words)\n",
    "    return p\n",
    "t = preprocessTesting([\"A fascinating insight into the life of modern Japanese teens: I thoroughly enjoyed Rising Sons and Daughters. I don't know of any other book that looks at Japanese society from the point of view of its young people poised as they are between their parents' age-old Japanese culture of restraint and obedience to the will of the community, and their peers' adulation of Western culture. True to form, the 'New Young' of Japan seem to be creating an 'international' blend, as the Ando family demonstrates in this beautifully written book of vignettes of the private lives of members of this family. Steven Wardell is clearly a talented young author, adopted for some of his schooling into this family of four teens, and thus able to view family life in Japan from the inside out. A great read!\"])\n",
    "print(t)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_vect = pickle.load(open(\"feature2.pkl\", \"rb\"))\n",
    "model = pickle.load(open(\"saved_model\", 'rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tfidf = load_vect.transform(t[\"new_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence Score for Class 1 =  0.017474610406846267\n",
      "Confidence Score for Class 2 =  0.9825253895931537\n"
     ]
    }
   ],
   "source": [
    "confidence = model.predict_proba(sent_tfidf)\n",
    "print(\"Confidence Score for Class 1 = \", confidence[0][0])\n",
    "print(\"Confidence Score for Class 2 = \", confidence[0][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
